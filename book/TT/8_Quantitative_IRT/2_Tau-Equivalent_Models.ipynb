{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e332052a",
   "metadata": {},
   "source": [
    "# 6.2 Tau-Equivalent Models\n",
    "## Essentially Tau-Equivalent Model\n",
    "\n",
    "The **Essentially tau-equivalent** measurement model is also quite flexible but it has one more restriction compared to the **Tau Congeneric** measurement model. It assumes that\n",
    "\n",
    "* items differ in their difficulty\n",
    "* items **are equivalent in their discrimination power**\n",
    "* items vary in their reliability  \n",
    "\n",
    "We therefore obtain estimates for the intercepts (`Intercepts` section) and for the errors (`Variances` section). A `Latent Variables` section is still present, but all loadings are fixed to 1.\n",
    "\n",
    "## Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1130add",
   "metadata": {},
   "source": [
    "#### Usage\n",
    "\n",
    "This notebook fits essentially tau-equivalent and tau-equivalent models to the `Data_EmotionalClarity.dat` items. After loading the prepared item matrix, the models are specified in `lavaan` and compared via fit indices. FIrst we load our packages and our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074f11b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_1</th>\n",
       "      <th>item_2</th>\n",
       "      <th>item_3</th>\n",
       "      <th>item_4</th>\n",
       "      <th>item_5</th>\n",
       "      <th>item_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>238.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.504005</td>\n",
       "      <td>1.422903</td>\n",
       "      <td>1.392156</td>\n",
       "      <td>1.304696</td>\n",
       "      <td>1.346359</td>\n",
       "      <td>1.305712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.359060</td>\n",
       "      <td>0.368799</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>0.407597</td>\n",
       "      <td>0.376931</td>\n",
       "      <td>0.383313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.201307</td>\n",
       "      <td>0.046884</td>\n",
       "      <td>0.047837</td>\n",
       "      <td>0.038259</td>\n",
       "      <td>0.162969</td>\n",
       "      <td>0.061095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.281558</td>\n",
       "      <td>1.185936</td>\n",
       "      <td>1.155308</td>\n",
       "      <td>1.052121</td>\n",
       "      <td>1.129379</td>\n",
       "      <td>1.075682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.525186</td>\n",
       "      <td>1.422746</td>\n",
       "      <td>1.369148</td>\n",
       "      <td>1.289783</td>\n",
       "      <td>1.347554</td>\n",
       "      <td>1.286473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.737302</td>\n",
       "      <td>1.651186</td>\n",
       "      <td>1.643980</td>\n",
       "      <td>1.551804</td>\n",
       "      <td>1.567729</td>\n",
       "      <td>1.557722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.437029</td>\n",
       "      <td>2.403697</td>\n",
       "      <td>2.455821</td>\n",
       "      <td>2.441999</td>\n",
       "      <td>2.408026</td>\n",
       "      <td>2.244108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           item_1      item_2      item_3      item_4      item_5      item_6\n",
       "count  238.000000  238.000000  238.000000  238.000000  238.000000  238.000000\n",
       "mean     1.504005    1.422903    1.392156    1.304696    1.346359    1.305712\n",
       "std      0.359060    0.368799    0.392299    0.407597    0.376931    0.383313\n",
       "min      0.201307    0.046884    0.047837    0.038259    0.162969    0.061095\n",
       "25%      1.281558    1.185936    1.155308    1.052121    1.129379    1.075682\n",
       "50%      1.525186    1.422746    1.369148    1.289783    1.347554    1.286473\n",
       "75%      1.737302    1.651186    1.643980    1.551804    1.567729    1.557722\n",
       "max      2.437029    2.403697    2.455821    2.441999    2.408026    2.244108"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Rpy2 imports\n",
    "from rpy2 import robjects as ro\n",
    "from rpy2.robjects import pandas2ri, numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Automatic conversion of arrays and dataframes\n",
    "pandas2ri.activate()\n",
    "numpy2ri.activate()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "ro.r('set.seed(123)')\n",
    "\n",
    "# Ipython extenrsion for magix plotting\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "# R imports\n",
    "importr('base')\n",
    "importr('lavaan')\n",
    "importr('psych')\n",
    "importr('stats')\n",
    "\n",
    "# Load data\n",
    "file_name = \"data/Data_EmotionalClarity.dat\"\n",
    "dat = pd.read_csv(file_name, sep=\"\\t\")\n",
    "dat2 = dat.iloc[:, 1:7]\n",
    "ro.globalenv['dat2'] = dat2\n",
    "dat2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lavaan 0.6-19 ended normally after 12 iterations\n",
      "\n",
      "  Estimator                                         ML\n",
      "  Optimization method                           NLMINB\n",
      "  Number of model parameters                        13\n",
      "\n",
      "  Number of observations                           238\n",
      "\n",
      "Model Test User Model:\n",
      "                                                      \n",
      "  Test statistic                                16.949\n",
      "  Degrees of freedom                                14\n",
      "  P-value (Chi-square)                           0.259\n",
      "\n",
      "Model Test Baseline Model:\n",
      "\n",
      "  Test statistic                               435.847\n",
      "  Degrees of freedom                                15\n",
      "  P-value                                        0.000\n",
      "\n",
      "User Model versus Baseline Model:\n",
      "\n",
      "  Comparative Fit Index (CFI)                    0.993\n",
      "  Tucker-Lewis Index (TLI)                       0.992\n",
      "\n",
      "Loglikelihood and Information Criteria:\n",
      "\n",
      "  Loglikelihood user model (H0)               -435.870\n",
      "  Loglikelihood unrestricted model (H1)       -427.396\n",
      "                                                      \n",
      "  Akaike (AIC)                                 897.740\n",
      "  Bayesian (BIC)                               942.880\n",
      "  Sample-size adjusted Bayesian (SABIC)        901.674\n",
      "\n",
      "Root Mean Square Error of Approximation:\n",
      "\n",
      "  RMSEA                                          0.030\n",
      "  90 Percent confidence interval - lower         0.000\n",
      "  90 Percent confidence interval - upper         0.073\n",
      "  P-value H_0: RMSEA <= 0.050                    0.737\n",
      "  P-value H_0: RMSEA >= 0.080                    0.023\n",
      "\n",
      "Standardized Root Mean Square Residual:\n",
      "\n",
      "  SRMR                                           0.053\n",
      "\n",
      "Parameter Estimates:\n",
      "\n",
      "  Standard errors                             Standard\n",
      "  Information                                 Expected\n",
      "  Information saturated (h1) model          Structured\n",
      "\n",
      "Latent Variables:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "  eta =~                                                                \n",
      "    item_1            1.000                               0.253    0.682\n",
      "    item_2            1.000                               0.253    0.689\n",
      "    item_3            1.000                               0.253    0.664\n",
      "    item_4            1.000                               0.253    0.656\n",
      "    item_5            1.000                               0.253    0.657\n",
      "    item_6            1.000                               0.253    0.650\n",
      "\n",
      "Intercepts:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "   .item_1            1.504    0.024   62.432    0.000    1.504    4.047\n",
      "   .item_2            1.423    0.024   59.697    0.000    1.423    3.870\n",
      "   .item_3            1.392    0.025   56.265    0.000    1.392    3.647\n",
      "   .item_4            1.305    0.025   52.077    0.000    1.305    3.376\n",
      "   .item_5            1.346    0.025   53.815    0.000    1.346    3.488\n",
      "   .item_6            1.306    0.025   51.677    0.000    1.306    3.350\n",
      "\n",
      "Variances:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "   .item_1            0.074    0.008    9.254    0.000    0.074    0.535\n",
      "   .item_2            0.071    0.008    9.186    0.000    0.071    0.525\n",
      "   .item_3            0.081    0.009    9.410    0.000    0.081    0.559\n",
      "   .item_4            0.085    0.009    9.475    0.000    0.085    0.570\n",
      "   .item_5            0.085    0.009    9.468    0.000    0.085    0.569\n",
      "   .item_6            0.088    0.009    9.518    0.000    0.088    0.577\n",
      "    eta               0.064    0.007    9.004    0.000    1.000    1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the model\n",
    "ro.r(\"mete = 'eta=~ item_1 + 1*item_2 + 1*item_3 + 1*item_4 + 1*item_5 + 1*item_6'\")\n",
    "# Fit the model\n",
    "ro.r('fitmete = sem(mete, data=dat2, meanstructure=TRUE)')\n",
    "# Print the output of the model for interpretation\n",
    "summary_fitmete = ro.r(\"summary(fitmete, fit.measures=TRUE, standardized=TRUE)\")\n",
    "print(summary_fitmete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70a662",
   "metadata": {},
   "source": [
    "You can see that the output looks very similar to that of the **Tau Congeneric** measurement model. Interpretation of the intercepts (`Intercepts` section) and the errors (`Variances` section) remains the same. The difference is that the loadings (`Latent Variables` section) are fixed to 1, implying equal discriminatory power for all items. Graphically, this results in parallel slopes across items. The fit indices are interpreted as before for the **Tau Congeneric** model.\n",
    "\n",
    "### Compare model fit\n",
    "\n",
    "Next, let's compare the models we just fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cae00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-Squared Difference Test\n",
      "\n",
      "        Df    AIC    BIC   Chisq Chisq diff    RMSEA Df diff Pr(>Chisq)\n",
      "fitmtc   9 900.36 962.86  9.5683                                       \n",
      "fitmete 14 897.74 942.88 16.9488     7.3805 0.044726       5     0.1938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recreate the tau-congeneric\n",
    "ro.r(\"mtc = 'eta =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6'\")\n",
    "ro.r('fitmtc = sem(mtc, data=dat2, meanstructure=TRUE)')\n",
    "\n",
    "# Essentially tau-equivalent model (notice the 1* in the model)\n",
    "ro.r(\"mete = 'eta=~ item_1 + 1*item_2 + 1*item_3 + 1*item_4 + 1*item_5 + 1*item_6'\")\n",
    "ro.r('fitmete = sem(mete, data=dat2, meanstructure=TRUE)')\n",
    "\n",
    "# Perform anova and print indexes\n",
    "anova_mete_mtc = ro.r(\"anova(fitmete, fitmtc)\")\n",
    "print(anova_mete_mtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eec3fa",
   "metadata": {},
   "source": [
    "According to the BIC and AIC the more restricted **Essentially tau-equivalent** model has a better model fit compared to the **Tau Congeneric** measurement model (as lower values for AIC and BIC indicate better model fit). The $\\chi^2$ Test however suggests that there are no significant differences in model fit as indicated by p > .05. This result is not too surprising as we already saw quite similar loading estimates across items in the **Tau Congeneric** measurement model (see [Tau-congeneric notebook](https://leonardozaggia.github.io/psy126/book/TT/8_Quantitative_IRT/1_Tau-Congeneric_Measurement_Model.html)). Therefore, restricting the loadings to equivalence isn't too much of a deviation from the **Tau Congeneric** measurement model (which does not restrict the loadings), resulting in a insignificant difference in model fit.\n",
    "\n",
    "## Tau-Equivalent Model\n",
    "\n",
    "The **Tau-equivalent** measurement model has one more restriction compared to the **Essentially tau-equivalent** model. It assumes that\n",
    "\n",
    "* items **are equivalent in their difficulty**\n",
    "* items **are equivalent in their discrimination power**\n",
    "* items vary in their reliability  \n",
    "\n",
    "We therefore obtain only the error estimates (`Variances` section). A `Latent Variables` and an `Intercepts` section still appear, but all loadings and intercepts are fixed.\n",
    "\n",
    "## Fit the model \n",
    "\n",
    "Since we need to code multiple lines in R at once, we will code such syntax as follows:\n",
    "\n",
    "```python\n",
    "# The model\n",
    "ro.r(\"\"\"\n",
    "      mte = 'eta =~ item_1 + 1*item_2 + 1*item_3 + 1*item_4 + 1*item_5 + 1*item_6\n",
    "      item_1 ~ a*1\n",
    "      item_2 ~ a*1\n",
    "      item_3 ~ a*1\n",
    "      item_4 ~ a*1\n",
    "      item_5 ~ a*1\n",
    "      item_6 ~ a*1'\n",
    "      \n",
    "      \"\"\")\n",
    "```\n",
    "\n",
    "everythin within the **\"\"\"** *r_code_here* **\"\"\"** will be passed as R code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba081c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lavaan 0.6-19 ended normally after 13 iterations\n",
      "\n",
      "  Estimator                                         ML\n",
      "  Optimization method                           NLMINB\n",
      "  Number of model parameters                        13\n",
      "  Number of equality constraints                     5\n",
      "\n",
      "  Number of observations                           238\n",
      "\n",
      "Model Test User Model:\n",
      "                                                      \n",
      "  Test statistic                               100.116\n",
      "  Degrees of freedom                                19\n",
      "  P-value (Chi-square)                           0.000\n",
      "\n",
      "Model Test Baseline Model:\n",
      "\n",
      "  Test statistic                               435.847\n",
      "  Degrees of freedom                                15\n",
      "  P-value                                        0.000\n",
      "\n",
      "User Model versus Baseline Model:\n",
      "\n",
      "  Comparative Fit Index (CFI)                    0.807\n",
      "  Tucker-Lewis Index (TLI)                       0.848\n",
      "\n",
      "Loglikelihood and Information Criteria:\n",
      "\n",
      "  Loglikelihood user model (H0)               -477.454\n",
      "  Loglikelihood unrestricted model (H1)       -427.396\n",
      "                                                      \n",
      "  Akaike (AIC)                                 970.908\n",
      "  Bayesian (BIC)                               998.686\n",
      "  Sample-size adjusted Bayesian (SABIC)        973.329\n",
      "\n",
      "Root Mean Square Error of Approximation:\n",
      "\n",
      "  RMSEA                                          0.134\n",
      "  90 Percent confidence interval - lower         0.109\n",
      "  90 Percent confidence interval - upper         0.160\n",
      "  P-value H_0: RMSEA <= 0.050                    0.000\n",
      "  P-value H_0: RMSEA >= 0.080                    1.000\n",
      "\n",
      "Standardized Root Mean Square Residual:\n",
      "\n",
      "  SRMR                                           0.111\n",
      "\n",
      "Parameter Estimates:\n",
      "\n",
      "  Standard errors                             Standard\n",
      "  Information                                 Expected\n",
      "  Information saturated (h1) model          Structured\n",
      "\n",
      "Latent Variables:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "  eta =~                                                                \n",
      "    item_1            1.000                               0.252    0.637\n",
      "    item_2            1.000                               0.252    0.683\n",
      "    item_3            1.000                               0.252    0.663\n",
      "    item_4            1.000                               0.252    0.639\n",
      "    item_5            1.000                               0.252    0.655\n",
      "    item_6            1.000                               0.252    0.634\n",
      "\n",
      "Intercepts:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "   .item_1     (a)    1.381    0.018   76.284    0.000    1.381    3.485\n",
      "   .item_2     (a)    1.381    0.018   76.284    0.000    1.381    3.736\n",
      "   .item_3     (a)    1.381    0.018   76.284    0.000    1.381    3.630\n",
      "   .item_4     (a)    1.381    0.018   76.284    0.000    1.381    3.496\n",
      "   .item_5     (a)    1.381    0.018   76.284    0.000    1.381    3.585\n",
      "   .item_6     (a)    1.381    0.018   76.284    0.000    1.381    3.471\n",
      "\n",
      "Variances:\n",
      "                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n",
      "   .item_1            0.093    0.010    9.529    0.000    0.093    0.594\n",
      "   .item_2            0.073    0.008    9.138    0.000    0.073    0.534\n",
      "   .item_3            0.081    0.009    9.318    0.000    0.081    0.560\n",
      "   .item_4            0.092    0.010    9.513    0.000    0.092    0.592\n",
      "   .item_5            0.085    0.009    9.387    0.000    0.085    0.571\n",
      "   .item_6            0.095    0.010    9.548    0.000    0.095    0.598\n",
      "    eta               0.064    0.007    8.880    0.000    1.000    1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the model\n",
    "ro.r(\"\"\"\n",
    "      mte = 'eta =~ item_1 + 1*item_2 + 1*item_3 + 1*item_4 + 1*item_5 + 1*item_6\n",
    "      item_1 ~ a*1\n",
    "      item_2 ~ a*1\n",
    "      item_3 ~ a*1\n",
    "      item_4 ~ a*1\n",
    "      item_5 ~ a*1\n",
    "      item_6 ~ a*1'\n",
    "      \n",
    "      \"\"\")\n",
    "# Fit the model\n",
    "ro.r('fitmte <- sem(mte, data=dat2, meanstructure=TRUE, estimator=\"ML\")')\n",
    "# Print the output of the model for interpretation\n",
    "summary_fitmte = ro.r(\"summary(fitmte, fit.measures=TRUE, standardized=TRUE)\")\n",
    "print(summary_fitmte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbcc35",
   "metadata": {},
   "source": [
    "Again, the output looks very similar to the previous ones. The interpretation also is equivalent to before. The only difference is that the loadings (`Latent Variables` section) and the intercept (`Intercepts` section) are fixed, meaning that we assume that all items have the same discriminatory power and the same difficulty. Graphically speaking, this means that the slopes and the intercepts of the items are equivalent. The interpretation of the fit indices is analogous to the **Tau Congeneric** measurement model (see above).\n",
    "\n",
    "### Compare model fit\n",
    "\n",
    "As before, we can use the `anova()` function to compare the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-Squared Difference Test\n",
      "\n",
      "        Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \n",
      "fitmete 14 897.74 942.88  16.949                                          \n",
      "fitmte  19 970.91 998.69 100.116     83.168 0.25629       5  < 2.2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform anova and print indexes\n",
    "anova_mete_mte = ro.r(\"anova(fitmete, fitmte)\")\n",
    "print(anova_mete_mte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e2f6a",
   "metadata": {},
   "source": [
    "In this comparison, the more restricted Tau-equivalent model has significantly worse fit compared to the Essentially tau-equivalent model as indicated by the significant differences in $\\chi^2$. Also AIC and BIC favor the more flexible model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
